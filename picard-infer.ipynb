{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "from sys import modules\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "import torch\n",
    "\n",
    "from typing import Optional, Dict\n",
    "from dataclasses import dataclass, field\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "from transformers.hf_argparser import HfArgumentParser\n",
    "from transformers.models.auto import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from uvicorn import run\n",
    "from sqlite3 import connect, OperationalError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpeakQL.Allennlp_models.utils.spider import process_sql, evaluation\n",
    "from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, \\\n",
    "    Postprocess_rewrite_seq, Postprocess_rewrite_seq_freeze_POS, Postprocess_rewrite_seq_modify_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/syt/Deep-Learning/Repos/picard\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/mac/Desktop/syt/Deep-Learning/Repos/picard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.utils.pipeline import Text2SQLGenerationPipeline, Text2SQLInput, get_schema\n",
    "from seq2seq.utils.picard_model_wrapper import PicardArguments, PicardLauncher, with_picard\n",
    "from seq2seq.utils.dataset import DataTrainingArguments\n",
    "from seq2seq.serve_seq2seq import BackendArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del modules['SpeakQL.Allennlp_models.utils.misc_utils']\n",
    "# del EvaluateSQL, EvaluateSQL_full, Postprocess_rewrite_seq\n",
    "# from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, Postprocess_rewrite_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Picard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/syt/Deep-Learning/Repos/picard/seq2seq\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/mac/Desktop/syt/Deep-Learning/Repos/picard/seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/picard\"\n",
    "\n",
    "args = {\n",
    "    \"model_path\": \"tscholak/3vnuv1vf\",\n",
    "    \"source_prefix\": \"\",\n",
    "    \"schema_serialization_type\": \"peteshaw\",\n",
    "    \"schema_serialization_randomized\": False,\n",
    "    \"schema_serialization_with_db_id\": True,\n",
    "    \"schema_serialization_with_db_content\": True,\n",
    "    \"normalize_query\": True,\n",
    "    \"target_with_db_id\": True,\n",
    "    \"db_path\": os.path.join(base_dir, \"database\"),\n",
    "    \"cache_dir\": os.path.join(base_dir, \"transformers_cache\"),\n",
    "    \"num_beams\": 4,\n",
    "    \"use_picard\": False,\n",
    "    \"launch_picard\": True,\n",
    "    \"picard_mode\": \"parse_with_guards\",\n",
    "    \"picard_schedule\": \"incremental\",\n",
    "    \"picard_max_tokens_to_check\": 2,\n",
    "    \"device\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': 'tscholak/3vnuv1vf',\n",
       " 'cache_dir': '/Users/mac/Desktop/syt/Deep-Learning/Repos/picard/transformers_cache',\n",
       " 'db_path': '/Users/mac/Desktop/syt/Deep-Learning/Repos/picard/database',\n",
       " 'host': '0.0.0.0',\n",
       " 'port': 8000,\n",
       " 'device': -1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend_args = BackendArguments()\n",
    "for k in backend_args.__dict__:\n",
    "    if k in args:\n",
    "        backend_args.__setattr__(k, args[k])\n",
    "backend_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overwrite_cache': False,\n",
       " 'preprocessing_num_workers': None,\n",
       " 'max_source_length': 512,\n",
       " 'max_target_length': 512,\n",
       " 'val_max_target_length': 512,\n",
       " 'val_max_time': None,\n",
       " 'max_train_samples': None,\n",
       " 'max_val_samples': None,\n",
       " 'num_beams': 4,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': None,\n",
       " 'ignore_pad_token_for_loss': True,\n",
       " 'source_prefix': '',\n",
       " 'schema_serialization_type': 'peteshaw',\n",
       " 'schema_serialization_randomized': False,\n",
       " 'schema_serialization_with_db_id': True,\n",
       " 'schema_serialization_with_db_content': True,\n",
       " 'normalize_query': True,\n",
       " 'target_with_db_id': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training_args = DataTrainingArguments()\n",
    "for k in data_training_args.__dict__:\n",
    "    if k in args:\n",
    "        data_training_args.__setattr__(k, args[k])\n",
    "data_training_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_picard': False,\n",
       " 'launch_picard': True,\n",
       " 'picard_host': 'localhost',\n",
       " 'picard_port': 9090,\n",
       " 'picard_mode': 'parse_with_guards',\n",
       " 'picard_schedule': 'incremental',\n",
       " 'picard_max_tokens_to_check': 2}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picard_args = PicardArguments()\n",
    "for k in picard_args.__dict__:\n",
    "    if k in args:\n",
    "        picard_args.__setattr__(k, args[k])\n",
    "picard_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    backend_args.model_path,\n",
    "    cache_dir=backend_args.cache_dir,\n",
    "    max_length=data_training_args.max_target_length,\n",
    "    num_beams=data_training_args.num_beams,\n",
    "    num_beam_groups=data_training_args.num_beam_groups,\n",
    "    diversity_penalty=data_training_args.diversity_penalty,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    backend_args.model_path,\n",
    "    cache_dir=backend_args.cache_dir,\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For now, not able to set picard = True (\"Picard not available\") \n",
    "if picard_args.use_picard:\n",
    "    model_cls_wrapper = lambda model_cls: with_picard(\n",
    "        model_cls=model_cls, picard_args=picard_args, tokenizer=tokenizer\n",
    "    )\n",
    "else:\n",
    "    model_cls_wrapper = lambda model_cls: model_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_cls_wrapper(AutoModelForSeq2SeqLM).from_pretrained(\n",
    "    backend_args.model_path,\n",
    "    config=config,\n",
    "    cache_dir=backend_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Text2SQLGenerationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    db_path=backend_args.db_path,\n",
    "    prefix=data_training_args.source_prefix,\n",
    "    normalize_query=data_training_args.normalize_query,\n",
    "    schema_serialization_type=data_training_args.schema_serialization_type,\n",
    "    schema_serialization_with_db_id=data_training_args.schema_serialization_with_db_id,\n",
    "    schema_serialization_with_db_content=data_training_args.schema_serialization_with_db_content,\n",
    "    device=backend_args.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picard predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Picard_predict(question, db_id):\n",
    "    outputs = pipe(inputs=Text2SQLInput(utterance=question, db_id=db_id))\n",
    "    output = outputs[0]['generated_text']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select count(*) from singer'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Picard_predict(\"how many singers do we have?\", \"concert_singer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"select employee_id, salary from employees where manager_id = (select employee_id from employees where first_name = 'pye' )\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Picard_predict(\"display the employee i D and salary of all employees who report to pye um, first name.\", \"hr_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select weight from pets where pettype = \"dog\" order by age asc limit 1'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Picard_predict('find the weight of the youngest dog.', 'pets_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict & Evaluate SQL given rewriter output / original / ASR cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1.0, 0), (0, 0.5, 1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvaluateSQL(pred_str='SELECT * FROM singer WHERE name = \"Joe Sharp\"',\n",
    "            gold_str='SELECT * FROM singer WHERE name = \"DEF\"',\n",
    "            db='concert_singer'), \\\n",
    "EvaluateSQL(pred_str='SELECT country FROM singer WHERE name = \"ABC\"',\n",
    "            gold_str='SELECT country FROM singer WHERE name = \"DEF\" ORDER BY age LIMIT 1',\n",
    "            db='concert_singer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic compares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter+phonemes.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # human test \n",
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "# orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4edc5f0a444c60a632a8665cb60405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select weight from pets where pettype = \"dog\" order by age asc limit 1\n",
      "SELECT weight FROM pets ORDER BY pet_age LIMIT 1\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct t2.petid) from has_pet as t1 join pets as t2 on t1.petid = t2.petid join student as t3 on t1.stuid = t3.stuid where t3.h > 20\n",
      "SELECT count(*) FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid WHERE T1.age  >  20\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.fname from student as t1 join has_pet as t2 on t1.stuid = t2.stuid join pets as t3 on t2.petid = t3.petid where t3.pettype = \"Captain Doug\" intersect select t1.fname from student as t1 join has_pet as t2 on t1.stuid = t2.stuid join pets as t3 on t2.petid = t3.petid where t3.pettype = \"Select t3.pettype from pets as t3 on t2.petid = t3.petid where t3.pettype = \"Select t3.pettype from pets as t4 on t2.petid = t4.petid where t3.pettype = \"Select t3.pettype from pets as t4 on t2.petid = t4.petid where t3.pettype = \"Select t\n",
      "SELECT T1.Fname FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat' INTERSECT SELECT T1.Fname FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'dog'\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select major, age from student where stuid not in (select stuid from has_pet where petid = t2.petid where pettype = \"cat\")\n",
      "SELECT major ,  age FROM student WHERE stuid NOT IN (SELECT T1.stuid FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat')\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select lname from student where stuid in (select stuid from has_pet where petid = t2.petid where age = 3)\n",
      "SELECT T1.lname FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pet_age  =  3 AND T3.pettype  =  'cat'\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select abbreviation from airlines where country = \"USA\" and check = \"Blue Airways\"\n",
      "SELECT Abbreviation FROM AIRLINES WHERE Airline  =  \"JetBlue Airways\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select airline from airlines where abbreviation = \"you'il\"\n",
      "SELECT Airline FROM AIRLINES WHERE Abbreviation  =  \"UAL\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select airline from airlines where abbreviation = \"You'll\"\n",
      "SELECT Airline FROM AIRLINES WHERE Abbreviation  =  \"UAL\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from flights where destination = \"Auto\"\n",
      "SELECT count(*) FROM FLIGHTS WHERE DestAirport  =  \"ATO\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from flights where auto = 'Yes'\n",
      "SELECT count(*) FROM FLIGHTS WHERE DestAirport  =  \"ATO\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from airlines as t1 join flights as t2 on t1.uid = t2.airline join airports as t3 on t2.airportcode = t3.airportcode where t3.city = 'Aberdeen' and t1.abbreviation = 'United Airlines'\n",
      "SELECT count(*) FROM FLIGHTS AS T1 JOIN AIRPORTS AS T2 ON T1.DestAirport  =  T2.AirportCode JOIN AIRLINES AS T3 ON T3.uid  =  T1.Airline WHERE T2.City  =  \"Aberdeen\" AND T3.Airline  =  \"United Airlines\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t3.city from flights as t1 join airports as t2 on t1.sourceairport = t2.airportcode join airlines as t3 on t1.destairport = t3.airportcode group by t3.city order by count(*) desc limit 1\n",
      "SELECT T1.City FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport GROUP BY T1.City ORDER BY count(*) DESC LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t3.city from flights as t1 join airports as t2 on t1.sourceairport = t2.airportcode join airlines as t3 on t1.destairport = t3.airportcode group by t3.city order by count(*) desc limit 1\n",
      "SELECT T1.City FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.SourceAirport GROUP BY T1.City ORDER BY count(*) DESC LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) desc limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) DESC LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) desc limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) DESC LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.abbreviation, t1.country from airlines as t1 join flights as t2 on t1.uid = t2.airline group by t2.abbreviation order by count(*) limit 1\n",
      "SELECT T1.Abbreviation ,  T1.Country FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid  =  T2.Airline GROUP BY T1.Airline ORDER BY count(*) LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t2.flightno from airports as t1 join flights as t2 on t1.airportcode = t2.airportcode where t1.country = \"USA\" and t2.sourceairport = \"PG\"\n",
      "SELECT FlightNo FROM FLIGHTS WHERE DestAirport  =  \"APG\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select airportname from airports where airportcode not in (select airportcode from flights)\n",
      "SELECT AirportName FROM Airports WHERE AirportCode NOT IN (SELECT SourceAirport FROM Flights UNION SELECT DestAirport FROM Flights)\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from documents where template_type_code = \"P\" and template_type_code = \"T\"\n",
      "SELECT count(*) FROM Documents AS T1 JOIN Templates AS T2 ON T1.Template_ID  =  T2.Template_ID WHERE T2.Template_Type_Code  =  'PPT'\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select t1.template_type_description, t1.template_type_code from templates as t1 join documents as t2 on t1.template_id = t2.template_id group by t1.template_type_code order by count(*) desc limit 1\n",
      "SELECT T1.template_id ,  T2.Template_Type_Code FROM Documents AS T1 JOIN Templates AS T2 ON T1.template_id  =  T2.template_id GROUP BY T1.template_id ORDER BY count(*) DESC LIMIT 1\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select t1.template_type_description, t1.template_type_code from templates as t1 join documents as t2 on t1.template_id = t2.template_id group by t1.template_type_code order by count(*) desc limit 1\n",
      "SELECT T1.template_id ,  T2.Template_Type_Code FROM Documents AS T1 JOIN Templates AS T2 ON T1.template_id  =  T2.template_id GROUP BY T1.template_id ORDER BY count(*) DESC LIMIT 1\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select avg(t1.odds) from templates as t1 join templates as t2 on t1.template_id = t2.template_id where t1.template_type_code = \"P P\" or t1.template_type_code = \"P P. T\"\n",
      "SELECT template_id FROM Templates WHERE template_type_code  =  \"PP\" OR template_type_code  =  \"PPT\"\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select template_id from templates where template_code = \"peop e\" or template_code = \"P P T\"\n",
      "SELECT template_id FROM Templates WHERE template_type_code  =  \"PP\" OR template_type_code  =  \"PPT\"\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.template_type_description from ref_template_types as t1 join documents as t2 on t1.template_type_code = t2.template_type_code\n",
      "SELECT DISTINCT T1.template_type_description FROM Ref_template_types AS T1 JOIN Templates AS T2 ON T1.template_type_code  = T2.template_type_code JOIN Documents AS T3 ON T2.Template_ID  =  T3.template_ID\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select t2.paragraph_text, t2.paragraph_id, t3.document_name from documents as t1 join paragraphs as t2 on t1.document_id = t2.document_id join paragraphs as t3 on t2.paragraph_id = t3.paragraph_id where t1.document_name = \"Welcome to NY\"\n",
      "SELECT T1.paragraph_id ,   T1.paragraph_text FROM Paragraphs AS T1 JOIN Documents AS T2 ON T1.document_id  =  T2.document_id WHERE T2.Document_Name  =  'Welcome to NY'\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select avg(t1.odds) from documents as t1 join paragraphs as t2 on t1.document_id = t2.document_id where t2.paragraph_text = \"Brazil\" intersect select avg(t1.odds) from documents as t1 join paragraphs as t2 on t1.document_id = t2.document_id where t2.paragraph_text = \"Ireland\"\n",
      "SELECT document_id FROM Paragraphs WHERE paragraph_text  =  'Brazil' INTERSECT SELECT document_id FROM Paragraphs WHERE paragraph_text  =  'Ireland'\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select agent from teacher\n",
      "SELECT Age ,  Hometown FROM teacher\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select agent from teacher\n",
      "SELECT Age ,  Hometown FROM teacher\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t2.name, t1.course from course_arrange as t1 join teacher as t2 on t1.teacher_id = t2.teacher_id\n",
      "SELECT T3.Name ,  T2.Course FROM course_arrange AS T1 JOIN course AS T2 ON T1.Course_ID  =  T2.Course_ID JOIN teacher AS T3 ON T1.Teacher_ID  =  T3.Teacher_ID\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t3.name from course_arrange as t1 join teacher as t2 on t1.teacher_id = t2.teacher_id join course as t3 on t1.course_id = t3.course_id order by t2.name asc\n",
      "SELECT T3.Name ,  T2.Course FROM course_arrange AS T1 JOIN course AS T2 ON T1.Course_ID  =  T2.Course_ID JOIN teacher AS T3 ON T1.Teacher_ID  =  T3.Teacher_ID ORDER BY T3.Name\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t2.name, t1.course from course_arrange as t1 join teacher as t2 on t1.teacher_id = t2.teacher_id order by t2.name asc\n",
      "SELECT T3.Name ,  T2.Course FROM course_arrange AS T1 JOIN course AS T2 ON T1.Course_ID  =  T2.Course_ID JOIN teacher AS T3 ON T1.Teacher_ID  =  T3.Teacher_ID ORDER BY T3.Name\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t2.name, t2.age from visit as t1 join visitor as t2 on t1.visit_id = t2.id group by t1.visit_id order by sum(t1.num_of_ticket) desc limit 1\n",
      "SELECT t1.name ,  t1.age FROM visitor AS t1 JOIN visit AS t2 ON t1.id  =  t2.visitor_id ORDER BY t2.num_of_ticket DESC LIMIT 1\n",
      "museum_visit\n",
      "process_sql.get_sql() failed\n",
      "select t2.name from visit as t1 join visitor as t2 on t1.visit_id = t2.id join museum as t3 on t1.museum_id = t3.museum_id where t3.open_year < 2009 intersect select t2.name from visit as t1 join visitor as t2 on t1.visit_id = t2.id join museum as t3 on t1.museum_id = t3.museum_id where t3.open_year > 2011\n",
      "SELECT t1.name FROM visitor AS t1 JOIN visit AS t2 ON t1.id  =  t2.visitor_id JOIN museum AS t3 ON t3.Museum_ID  =  t2.Museum_ID WHERE t3.open_year  <  2009 INTERSECT SELECT t1.name FROM visitor AS t1 JOIN visit AS t2 ON t1.id  =  t2.visitor_id JOIN museum AS t3 ON t3.Museum_ID  =  t2.Museum_ID WHERE t3.open_year  >  2011\n",
      "museum_visit\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from visitor where id not in ( select visitor_id from visit where open_year > 2010 )\n",
      "SELECT count(*) FROM visitor WHERE id NOT IN (SELECT t2.visitor_id FROM museum AS t1 JOIN visit AS t2 ON t1.Museum_ID  =  t2.Museum_ID WHERE t1.open_year  >  2010)\n",
      "museum_visit\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name from matches as t1 join players as t2 on t1.player_id = t2.player_id where t2.player_name = 'Terni' group by t1.player_id having count(*) > 10\n",
      "SELECT tourney_name FROM matches GROUP BY tourney_name HAVING count(*)  >  10\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.first_name, t1.last_name from players as t1 join matches as t2 on t1.player_id = t2.player_id where t2.year = 2013 intersect select t1.first_name, t1.last_name from players as t1 join matches as t2 on t1.player_id = t2.player_id where t2.year = 2016\n",
      "SELECT winner_name FROM matches WHERE YEAR  =  2013 INTERSECT SELECT winner_name FROM matches WHERE YEAR  =  2016\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.country_code, t1.first_name from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"Atlantic Open\" intersect select t1.country_code, t1.first_name from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"Australian Open\"\n",
      "SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'WTA Championships' INTERSECT SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'Australian Open'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.first_name, t1.country_code from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"W th Championships\" intersect select t1.first_name, t1.country_code from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"Australian Open\"\n",
      "SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'WTA Championships' INTERSECT SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'Australian Open'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name, t1.winner_rank_points from winners as t1 join rankings as t2 on t1.winner_id = t2.player_id group by t2.player_id order by count(*) desc limit 1\n",
      "SELECT winner_name ,  winner_rank_points FROM matches GROUP BY winner_name ORDER BY count(*) DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name, t1.winning_rank_points from matches as t1 join rankings as t2 on t1.winner_id = t2.player_id group by t2.player_id order by count(*) desc limit 1\n",
      "SELECT winner_name ,  winner_rank_points FROM matches GROUP BY winner_name ORDER BY count(*) DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name from matches as t1 join rankings as t2 on t1.winner_id = t2.player_id join tourneys as t3 on t2.tournament_id = t3.tournament_id where t3.tournament_name = \"Australian Open\" order by t2.winner_rank_points desc limit 1\n",
      "SELECT winner_name FROM matches WHERE tourney_name  =  'Australian Open' ORDER BY winner_rank_points DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name from matches as t1 join rankings as t2 on t1.winner_id = t2.player_id join tourneys as t3 on t2.tournament_id = t3.tournament_id where t3.tournament_name = \"Australian Open\" order by t2.winner_rank_points desc limit 1\n",
      "SELECT winner_name FROM matches WHERE tourney_name  =  'Australian Open' ORDER BY winner_rank_points DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name, t1.loser_name from winners as t1 join matches as t2 on t1.winner_id = t2.winner_id order by t2.minutes desc limit 1\n",
      "SELECT winner_name ,  loser_name FROM matches ORDER BY minutes DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select avg(t1.ranking), t1.first_name from rankings as t1 join players as t2 on t1.player_id = t2.player_id group by t1.first_name\n",
      "SELECT avg(ranking) ,  T1.first_name FROM players AS T1 JOIN rankings AS T2 ON T1.player_id  =  T2.player_id GROUP BY T1.first_name\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct t1.winner_name) from matches as t1 join participants as t2 on t1.winner_id = t2.winner_id where t2.tournament_name = \"W\" intersect select count(distinct t1.winner_name) from matches as t1 join participants as t2 on t1.winner_id = t2.winner_id where t2.hand = \"left\"\n",
      "SELECT count(DISTINCT winner_name) FROM matches WHERE tourney_name  =  'WTA Championships' AND winner_hand  =  'L'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from participants as t1 join matches as t2 on t1.player_id = t2.winner_id where t1.hand = \"left\" and t2.tournament_name = \"W\"\n",
      "SELECT count(DISTINCT winner_name) FROM matches WHERE tourney_name  =  'WTA Championships' AND winner_hand  =  'L'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.first_name, t1.country_code, t1.birth_date from players as t1 join rankings as t2 on t1.player_id = t2.player_id group by t2.player_id order by sum(t2.winner_rank_points) desc limit 1\n",
      "SELECT T1.first_name ,  T1.country_code ,  T1.birth_date FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id ORDER BY T2.winner_rank_points DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select t1.area_code from area_code_state as t1 join votes as t2 on t1.area_code = t2.area_code group by t1.area_code order by count(*) desc limit 1\n",
      "SELECT T1.area_code FROM area_code_state AS T1 JOIN votes AS T2 ON T1.state  =  T2.state GROUP BY T1.area_code ORDER BY count(*) DESC LIMIT 1\n",
      "voter_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.area_code from area_code_state as t1 join votes as t2 on t1.area_code = t2.area_code join contestants as t3 on t2.contestant_number = t3.contestant_number where t3.contestant_name = 'Tabatha' intersect select t1.area_code from area_code_state as t1 join votes as t2 on t1.area_code = t2.area_code join contestants as t3 on t2.contestant_number = t3.contestant_number where t3.contestant_name = 'Kelly Kloss'\n",
      "SELECT T3.area_code FROM contestants AS T1 JOIN votes AS T2 ON T1.contestant_number  =  T2.contestant_number JOIN area_code_state AS T3 ON T2.state  =  T3.state WHERE T1.contestant_name  =  'Tabatha Gehling' INTERSECT SELECT T3.area_code FROM contestants AS T1 JOIN votes AS T2 ON T1.contestant_number  =  T2.contestant_number JOIN area_code_state AS T3 ON T2.state  =  T3.state WHERE T1.contestant_name  =  'Kelly Clauss'\n",
      "voter_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.region from city as t1 join country as t2 on t1.id = t2.countrycode where t1.district = \"Kabol\"\n",
      "SELECT Region FROM country AS T1 JOIN city AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Name  =  \"Kabul\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.region from city as t1 join country as t2 on t1.id = t2.countrycode where t1.name = \"Kabul\"\n",
      "SELECT Region FROM country AS T1 JOIN city AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Name  =  \"Kabul\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\" AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\" AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct t1.continent) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = \"Chinese\"\n",
      "SELECT COUNT( DISTINCT Continent) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"Chinese\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct t1.continent) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = \"Chinese\"\n",
      "SELECT COUNT( DISTINCT Continent) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"Chinese\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.region from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = \"English\" or t2.language = \"Dutch\"\n",
      "SELECT DISTINCT T1.Region FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" OR T2.Language  =  \"Dutch\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select region from country where language = 'English'\n",
      "SELECT DISTINCT T1.Region FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" OR T2.Language  =  \"Dutch\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select avg(lifeexpectancy) from country where countrycode not in (select countrycode from countrylanguage where language = 'English')\n",
      "SELECT avg(LifeExpectancy) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select avg(lifeexpectancy) from country where countrycode not in (select t1.countrycode from countrylanguage as t1 join country as t2 on t1.isofficial = t2.countrycode where t2.language = 'English')\n",
      "SELECT avg(LifeExpectancy) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select sum(population) from country where countrycode not in (select countrycode from countrylanguage where language = 'English')\n",
      "SELECT sum(Population) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from country where countrycode not in (select countrycode from countrylanguage where language = 'English')\n",
      "SELECT sum(Population) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct language) from countrylanguage where indepyear < 1930\n",
      "SELECT count(DISTINCT T2.Language) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE  IndepYear  <  1930 AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct language) from countrylanguage where indepyear < 1930\n",
      "SELECT count(DISTINCT T2.Language) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE  IndepYear  <  1930 AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from country where language != 'English'\n",
      "SELECT CountryCode FROM countrylanguage EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from country where language != 'English'\n",
      "SELECT CountryCode FROM countrylanguage EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from country where language != \"English\"\n",
      "SELECT DISTINCT CountryCode FROM countrylanguage WHERE LANGUAGE != \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select name from country where countrycode not in (select countrycode from countrylanguage where language = 'English') and governmentform != 'Republic'\n",
      "SELECT Code FROM country WHERE GovernmentForm != \"Republic\" EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select name from country where countrycode not in (select countrycode from countrylanguage where language = 'English') and governmentform != 'Republic'\n",
      "SELECT Code FROM country WHERE GovernmentForm != \"Republic\" EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from city as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.continent = 'Europe' and t2.isofficial != 'English'\n",
      "SELECT DISTINCT T2.Name FROM country AS T1 JOIN city AS T2 ON T2.CountryCode  =  T1.Code WHERE T1.Continent  =  'Europe' AND T1.Name NOT IN (SELECT T3.Name FROM country AS T3 JOIN countrylanguage AS T4 ON T3.Code  =  T4.CountryCode WHERE T4.IsOfficial  =  'T' AND T4.Language  =  'English')\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.name from city as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.continent = \"Asia\" and t2.language = \"Chinese\"\n",
      "SELECT DISTINCT T3.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode JOIN city AS T3 ON T1.Code  =  T3.CountryCode WHERE T2.IsOfficial  =  'T' AND T2.Language  =  'Chinese' AND T1.Continent  =  \"Asia\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.name from city as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.continent = 'Asia' and t2.language = 'Chinese'\n",
      "SELECT DISTINCT T3.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode JOIN city AS T3 ON T1.Code  =  T3.CountryCode WHERE T2.IsOfficial  =  'T' AND T2.Language  =  'Chinese' AND T1.Continent  =  \"Asia\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name, count(*) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode group by t1.countrycode having count(*) >= 3\n",
      "SELECT COUNT(T2.Language) ,  T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode GROUP BY T1.Name HAVING COUNT(*)  >  2\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name, count(*) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode group by t1.countrycode having count(*) > 2\n",
      "SELECT COUNT(T2.Language) ,  T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode GROUP BY T1.Name HAVING COUNT(*)  >  2\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select name from country where continent = 'Europe' and population = 80,000\n",
      "SELECT Name FROM country WHERE continent  =  \"Europe\" AND Population  =  \"80000\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select name from country where continent = 'Europe' and population = 80,000\n",
      "SELECT Name FROM country WHERE continent  =  \"Europe\" AND Population  =  \"80000\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode, language, sum(percent) from countrylanguage group by countrycode order by sum(percent) desc limit 1\n",
      "SELECT LANGUAGE ,  CountryCode ,  max(Percentage) FROM countrylanguage GROUP BY CountryCode\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from countrylanguage where language = \"Spanish\" group by countrycode order by sum(percent) desc limit 1\n",
      "SELECT count(*) ,   max(Percentage) FROM countrylanguage WHERE LANGUAGE  =  \"Spanish\" GROUP BY CountryCode\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from countrylanguage where language = \"Spanish\" group by countrycode order by sum(percent) desc limit 1\n",
      "SELECT CountryCode ,  max(Percentage) FROM countrylanguage WHERE LANGUAGE  =  \"Spanish\" GROUP BY CountryCode\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name, t2.orchestra from conductor as t1 join orchestra as t2 on t1.coach_id = t2.coach_id where t1.name = \"Chorus\"\n",
      "SELECT T1.Name ,  T2.Orchestra FROM conductor AS T1 JOIN orchestra AS T2 ON T1.Conductor_ID  =  T2.Conductor_ID\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "select Mets from orchestra group by Mets order by count(*) asc\n",
      "SELECT Major_Record_Format FROM orchestra GROUP BY Major_Record_Format ORDER BY COUNT(*) ASC\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "select t2.grade from highschooler as t1 join likes as t2 on t1.id = t2.student_id where t1.name = \"Kyle\"\n",
      "SELECT grade FROM Highschooler WHERE name  =  \"Kyle\"\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.theme from highschooler as t1 join likes as t2 on t1.id = t2.student_id where t1.name = \"Kyle\"\n",
      "SELECT ID FROM Highschooler WHERE name  =  \"Kyle\"\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select idee from highschooler where name = 'Kyle'\n",
      "SELECT ID FROM Highschooler WHERE name  =  \"Kyle\"\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.name from highschooler as t1 join friend as t2 on t1.id = t2.friend_id where t1.name = 'Kyle'\n",
      "SELECT T3.name FROM Friend AS T1 JOIN Highschooler AS T2 ON T1.student_id  =  T2.id JOIN Highschooler AS T3 ON T1.friend_id  =  T3.id WHERE T2.name  =  \"Kyle\"\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.name from highschooler as t1 join friend as t2 on t1.id = t2.friend_id where t1.name = \"Kyle\"\n",
      "SELECT T3.name FROM Friend AS T1 JOIN Highschooler AS T2 ON T1.student_id  =  T2.id JOIN Highschooler AS T3 ON T1.friend_id  =  T3.id WHERE T2.name  =  \"Kyle\"\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select avg(t1.od) from highschooler as t1 join friend as t2 on t1.id = t2.student_id\n",
      "SELECT id FROM Highschooler EXCEPT SELECT student_id FROM Friend\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select avg(t1.odds) from likes as t1 join friend as t2 on t1.student_id = t2.student_id intersect select avg(t1.odds) from likes as t1 join student as t2 on t1.student_id = t2.student_id\n",
      "SELECT student_id FROM Friend INTERSECT SELECT liked_id FROM Likes\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*), t1.student_id from likes as t1 join students as t2 on t1.student_id = t2.student_id group by t1.student_id\n",
      "SELECT student_id ,  count(*) FROM Likes GROUP BY student_id\n",
      "network_1\n",
      "process_sql.get_sql() failed\n"
     ]
    }
   ],
   "source": [
    "# Just using the 1st ASR candidate, no correction \n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Picard_predict(c['question'], _db_id)\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "    c['exec'] = _exec\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['question'], _db_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.6884\n",
      "avg_exact = 0.5539\n",
      "avg_exec = 0.5686\n",
      "BLEU = 0.8010\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "# ## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "# print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "#                  glist=[d[0]['query'] for d in test_dataset],\n",
    "#                  db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "#                  etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the predictions \n",
    "output_dir = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs'\n",
    "# Orig\n",
    "picard_out_dir = os.path.join(output_dir, 'picard-test-save')\n",
    "os.makedirs(picard_out_dir, exist_ok=True)\n",
    "test_output_path = os.path.join(picard_out_dir, 'First-cands.json')\n",
    "\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/Assembly/First-cands.json'\n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/Assembly/First-cands.json'\n",
    "# Mixed\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/First-cands.json'\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/Assembly/First-cands.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 3075, 1034)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae964f37ea141deaea5270f0fb20ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select weight from pets where pettype = \"dog\" order by age asc limit 1\n",
      "SELECT weight FROM pets ORDER BY pet_age LIMIT 1\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select weight from pets order by age asc limit 1\n",
      "SELECT weight FROM pets ORDER BY pet_age LIMIT 1\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select major, age from student where stuid not in (select stuid from has_pet where pettype = \"cat\")\n",
      "SELECT major ,  age FROM student WHERE stuid NOT IN (SELECT T1.stuid FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat')\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select stuid from student except select stuid from has_pet where pettype = \"cat\"\n",
      "SELECT stuid FROM student EXCEPT SELECT T1.stuid FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat'\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select stuid from student except select stuid from has_pet where pettype = \"cat\"\n",
      "SELECT stuid FROM student EXCEPT SELECT T1.stuid FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat'\n",
      "pets_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from airports as t1 join flights as t2 on t1.airport = t2.sourceairport where t1.airportcode = \"ATO\"\n",
      "SELECT count(*) FROM FLIGHTS WHERE DestAirport  =  \"ATO\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from airports as t1 join flights as t2 on t1.airportcode = t2.airportcode where t1.airportcode = \"ATO\"\n",
      "SELECT count(*) FROM FLIGHTS WHERE DestAirport  =  \"ATO\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from airlines as t1 join flights as t2 on t1.uid = t2.airline where t1.airline = 'United Airlines' and t2.airportcode = 'ASY'\n",
      "SELECT count(*) FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T2.Airline  =  T1.uid WHERE T1.Airline  =  \"United Airlines\" AND T2.DestAirport  =  \"ASY\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from airlines as t1 join flights as t2 on t1.uid = t2.airline where t1.airline = \"United Airlines\" and t2.airportcode = \"ASY\"\n",
      "SELECT count(*) FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T2.Airline  =  T1.uid WHERE T1.Airline  =  \"United Airlines\" AND T2.DestAirport  =  \"ASY\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) desc limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) DESC LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) desc limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) DESC LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.airportcode from airports as t1 join flights as t2 on t1.airport = t2.sourceairport group by t2.sourceairport order by count(*) limit 1\n",
      "SELECT T1.AirportCode FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode  =  T2.DestAirport OR T1.AirportCode  =  T2.SourceAirport GROUP BY T1.AirportCode ORDER BY count(*) LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.abbreviation, t1.country from airlines as t1 join flights as t2 on t1.uid = t2.airline group by t2.abbreviation order by count(*) limit 1\n",
      "SELECT T1.Abbreviation ,  T1.Country FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid  =  T2.Airline GROUP BY T1.Airline ORDER BY count(*) LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t1.abbreviation, t1.country from airlines as t1 join flights as t2 on t1.uid = t2.airline group by t2.abbreviation order by count(*) limit 1\n",
      "SELECT T1.Abbreviation ,  T1.Country FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid  =  T2.Airline GROUP BY T1.Airline ORDER BY count(*) LIMIT 1\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select t2.flightno from airports as t1 join flights as t2 on t1.airportcode = t2.agp where t1.airportcode = \"APG\"\n",
      "SELECT FlightNo FROM FLIGHTS WHERE DestAirport  =  \"APG\"\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select airportname from airports where airportcode not in (select airportcode from flights)\n",
      "SELECT AirportName FROM Airports WHERE AirportCode NOT IN (SELECT SourceAirport FROM Flights UNION SELECT DestAirport FROM Flights)\n",
      "flight_2\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.template_type_description from ref_template_types as t1 join documents as t2 on t1.template_type_code = t2.template_type_code\n",
      "SELECT DISTINCT T1.template_type_description FROM Ref_template_types AS T1 JOIN Templates AS T2 ON T1.template_type_code  = T2.template_type_code JOIN Documents AS T3 ON T2.Template_ID  =  T3.template_ID\n",
      "cre_Doc_Template_Mgt\n",
      "process_sql.get_sql() failed\n",
      "select t2.name, t1.course from course_arrange as t1 join teacher as t2 on t1.teacher_id = t2.teacher_id\n",
      "SELECT T3.Name ,  T2.Course FROM course_arrange AS T1 JOIN course AS T2 ON T1.Course_ID  =  T2.Course_ID JOIN teacher AS T3 ON T1.Teacher_ID  =  T3.Teacher_ID\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t2.name, t1.course from course_arrange as t1 join teacher as t2 on t1.teacher_id = t2.teacher_id\n",
      "SELECT T3.Name ,  T2.Course FROM course_arrange AS T1 JOIN course AS T2 ON T1.Course_ID  =  T2.Course_ID JOIN teacher AS T3 ON T1.Teacher_ID  =  T3.Teacher_ID\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t2.name, t1.course from course_arrange as t1 join teacher as t2 on t1.teacher_id = t2.teacher_id order by t2.name asc\n",
      "SELECT T3.Name ,  T2.Course FROM course_arrange AS T1 JOIN course AS T2 ON T1.Course_ID  =  T2.Course_ID JOIN teacher AS T3 ON T1.Teacher_ID  =  T3.Teacher_ID ORDER BY T3.Name\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t2.name, t1.course from course_arrange as t1 join teacher as t2 on t1.teacher_id = t2.teacher_id order by t2.name asc\n",
      "SELECT T3.Name ,  T2.Course FROM course_arrange AS T1 JOIN course AS T2 ON T1.Course_ID  =  T2.Course_ID JOIN teacher AS T3 ON T1.Teacher_ID  =  T3.Teacher_ID ORDER BY T3.Name\n",
      "course_teach\n",
      "process_sql.get_sql() failed\n",
      "select t2.name from visit as t1 join visitor as t2 on t1.visit_id = t2.id join museum as t3 on t1.museum_id = t3.museum_id where t3.open_year < 2009 intersect select t2.name from visit as t1 join visitor as t2 on t1.visit_id = t2.id join museum as t3 on t1.museum_id = t3.museum_id where t3.open_year > 2011\n",
      "SELECT t1.name FROM visitor AS t1 JOIN visit AS t2 ON t1.id  =  t2.visitor_id JOIN museum AS t3 ON t3.Museum_ID  =  t2.Museum_ID WHERE t3.open_year  <  2009 INTERSECT SELECT t1.name FROM visitor AS t1 JOIN visit AS t2 ON t1.id  =  t2.visitor_id JOIN museum AS t3 ON t3.Museum_ID  =  t2.Museum_ID WHERE t3.open_year  >  2011\n",
      "museum_visit\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from visitor where id not in ( select visitor_id from visit where open_year > 2010 )\n",
      "SELECT count(*) FROM visitor WHERE id NOT IN (SELECT t2.visitor_id FROM museum AS t1 JOIN visit AS t2 ON t1.Museum_ID  =  t2.Museum_ID WHERE t1.open_year  >  2010)\n",
      "museum_visit\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.year = 2013 intersect select t1.winner_name from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.year = 2016\n",
      "SELECT winner_name FROM matches WHERE YEAR  =  2013 INTERSECT SELECT winner_name FROM matches WHERE YEAR  =  2016\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.country_code, t1.first_name from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"WTA Championships\" intersect select t1.country_code, t1.first_name from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"Australian Open\"\n",
      "SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'WTA Championships' INTERSECT SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'Australian Open'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select t1.first_name, t1.country_code from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"WTA Championships\" intersect select t1.first_name, t1.country_code from players as t1 join matches as t2 on t1.player_id = t2.winner_id where t2.tournament_name = \"Australian Open\"\n",
      "SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'WTA Championships' INTERSECT SELECT T1.country_code ,  T1.first_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id WHERE T2.tourney_name  =  'Australian Open'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name, t1.winner_rank_points from winners as t1 join rankings as t2 on t1.winner_id = t2.player_id group by t2.player_id order by count(*) desc limit 1\n",
      "SELECT winner_name ,  winner_rank_points FROM matches GROUP BY winner_name ORDER BY count(*) DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name, t1.winning_rank_points from matches as t1 join rankings as t2 on t1.winner_id = t2.player_id group by t2.player_id order by count(*) desc limit 1\n",
      "SELECT winner_name ,  winner_rank_points FROM matches GROUP BY winner_name ORDER BY count(*) DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name from matches as t1 join rankings as t2 on t1.winner_id = t2.player_id join tourneys as t3 on t2.tournament_id = t3.tournament_id where t3.tournament_name = \"Australian Open\" order by t2.winner_rank_points desc limit 1\n",
      "SELECT winner_name FROM matches WHERE tourney_name  =  'Australian Open' ORDER BY winner_rank_points DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name from matches as t1 join rankings as t2 on t1.winner_id = t2.player_id join tourneys as t3 on t2.tournament_id = t3.tournament_id where t3.tournament_name = \"Australian Open\" order by t2.winner_rank_points desc limit 1\n",
      "SELECT winner_name FROM matches WHERE tourney_name  =  'Australian Open' ORDER BY winner_rank_points DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.loser_name, t2.winner_name from matches as t1 join players as t2 on t1.loser_id = t2.player_id join winners as t3 on t1.winner_id = t3.winner_id where t1.minutes = (select max(minutes) from matches)\n",
      "SELECT winner_name ,  loser_name FROM matches ORDER BY minutes DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.winner_name, t1.loser_name from winners as t1 join matches as t2 on t1.winner_id = t2.winner_id order by t2.minutes desc limit 1\n",
      "SELECT winner_name ,  loser_name FROM matches ORDER BY minutes DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct t1.winner_name) from matches as t1 join players as t2 on t1.winner_id = t2.player_id where t1.tournament_name = \"WTA Championships\" intersect select count(distinct t1.winner_name) from matches as t1 join players as t2 on t1.winner_id = t2.player_id where t1.tournament_name = \"Left Hand\"\n",
      "SELECT count(DISTINCT winner_name) FROM matches WHERE tourney_name  =  'WTA Championships' AND winner_hand  =  'L'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from matches as t1 join players as t2 on t1.winner_id = t2.player_id where t1.tournament_name = \"WTA Championships\" and t2.hand = \"left\"\n",
      "SELECT count(DISTINCT winner_name) FROM matches WHERE tourney_name  =  'WTA Championships' AND winner_hand  =  'L'\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.first_name, t1.country_code, t1.birth_date from players as t1 join rankings as t2 on t1.player_id = t2.player_id group by t2.player_id order by sum(t2.winner_rank_points) desc limit 1\n",
      "SELECT T1.first_name ,  T1.country_code ,  T1.birth_date FROM players AS T1 JOIN matches AS T2 ON T1.player_id  =  T2.winner_id ORDER BY T2.winner_rank_points DESC LIMIT 1\n",
      "wta_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.area_code from area_code_state as t1 join votes as t2 on t1.area_code = t2.area_code group by t1.area_code order by count(*) desc limit 1\n",
      "SELECT T1.area_code FROM area_code_state AS T1 JOIN votes AS T2 ON T1.state  =  T2.state GROUP BY T1.area_code ORDER BY count(*) DESC LIMIT 1\n",
      "voter_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.area_code from area_code_state as t1 join votes as t2 on t1.area_code = t2.area_code join contestants as t3 on t2.contestant_number = t3.contestant_number where t3.contestant_name = 'Tabatha Gehling' intersect select t1.area_code from area_code_state as t1 join votes as t2 on t1.area_code = t2.area_code join contestants as t3 on t2.contestant_number = t3.contestant_number where t3.contestant_name = 'Kelly Clauss'\n",
      "SELECT T3.area_code FROM contestants AS T1 JOIN votes AS T2 ON T1.contestant_number  =  T2.contestant_number JOIN area_code_state AS T3 ON T2.state  =  T3.state WHERE T1.contestant_name  =  'Tabatha Gehling' INTERSECT SELECT T3.area_code FROM contestants AS T1 JOIN votes AS T2 ON T1.contestant_number  =  T2.contestant_number JOIN area_code_state AS T3 ON T2.state  =  T3.state WHERE T1.contestant_name  =  'Kelly Clauss'\n",
      "voter_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.region from city as t1 join country as t2 on t1.id = t2.countrycode where t1.name = \"Kabul\"\n",
      "SELECT Region FROM country AS T1 JOIN city AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Name  =  \"Kabul\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.region from city as t1 join country as t2 on t1.id = t2.countrycode where t1.name = \"Kabul\"\n",
      "SELECT Region FROM country AS T1 JOIN city AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Name  =  \"Kabul\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode group by t2.countrycode order by count(*) desc limit 1\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode GROUP BY T1.Name ORDER BY COUNT(*) DESC LIMIT 1\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\" AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'English' intersect select t1.name from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = 'French'\n",
      "SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"French\" AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct t1.continent) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = \"Chinese\"\n",
      "SELECT COUNT( DISTINCT Continent) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"Chinese\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select count(distinct t1.continent) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = \"Chinese\"\n",
      "SELECT COUNT( DISTINCT Continent) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"Chinese\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.region from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = \"English\" or t2.language = \"Dutch\"\n",
      "SELECT DISTINCT T1.Region FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" OR T2.Language  =  \"Dutch\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.region from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.language = \"Dutch\" or t2.language = \"English\"\n",
      "SELECT DISTINCT T1.Region FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" OR T2.Language  =  \"Dutch\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select avg(lifeexpectancy) from country where countrycode not in (select countrycode from countrylanguage where language = 'English')\n",
      "SELECT avg(LifeExpectancy) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select avg(lifeexpectancy) from country where countrycode not in (select countrycode from countrylanguage where language = 'English')\n",
      "SELECT avg(LifeExpectancy) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\" AND T2.IsOfficial  =  \"T\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select sum(population) from country where countrycode not in (select countrycode from countrylanguage where language = 'English')\n",
      "SELECT sum(Population) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from country where countrycode not in (select countrycode from countrylanguage where language = 'English')\n",
      "SELECT sum(Population) FROM country WHERE Name NOT IN (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE T2.Language  =  \"English\")\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct language) from countrylanguage where indepyear < 1930\n",
      "SELECT count(DISTINCT T2.Language) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE  IndepYear  <  1930 AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct language) from countrylanguage where indepyear < 1930\n",
      "SELECT count(DISTINCT T2.Language) FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode WHERE  IndepYear  <  1930 AND T2.IsOfficial  =  \"T\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from country where language != 'English'\n",
      "SELECT CountryCode FROM countrylanguage EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from country where language != 'English'\n",
      "SELECT CountryCode FROM countrylanguage EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from country where language != \"English\" and governmentform != \"Republic\"\n",
      "SELECT Code FROM country WHERE GovernmentForm != \"Republic\" EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from country where language != \"English\" and governmentform != \"Republic\"\n",
      "SELECT Code FROM country WHERE GovernmentForm != \"Republic\" EXCEPT SELECT CountryCode FROM countrylanguage WHERE LANGUAGE  =  \"English\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name from city as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.continent = 'Europe' and t2.isofficial != 'English'\n",
      "SELECT DISTINCT T2.Name FROM country AS T1 JOIN city AS T2 ON T2.CountryCode  =  T1.Code WHERE T1.Continent  =  'Europe' AND T1.Name NOT IN (SELECT T3.Name FROM country AS T3 JOIN countrylanguage AS T4 ON T3.Code  =  T4.CountryCode WHERE T4.IsOfficial  =  'T' AND T4.Language  =  'English')\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(distinct t1.name) from city as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.continent = \"Asia\" and t2.language = \"Chinese\"\n",
      "SELECT DISTINCT T3.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode JOIN city AS T3 ON T1.Code  =  T3.CountryCode WHERE T2.IsOfficial  =  'T' AND T2.Language  =  'Chinese' AND T1.Continent  =  \"Asia\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select distinct t1.name from city as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode where t2.continent = 'Asia' and t2.language = 'Chinese'\n",
      "SELECT DISTINCT T3.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode JOIN city AS T3 ON T1.Code  =  T3.CountryCode WHERE T2.IsOfficial  =  'T' AND T2.Language  =  'Chinese' AND T1.Continent  =  \"Asia\"\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name, count(*) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode group by t1.countrycode having count(*) >= 3\n",
      "SELECT COUNT(T2.Language) ,  T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode GROUP BY T1.Name HAVING COUNT(*)  >  2\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name, count(*) from country as t1 join countrylanguage as t2 on t1.countrycode = t2.countrycode group by t1.countrycode having count(*) > 2\n",
      "SELECT COUNT(T2.Language) ,  T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code  =  T2.CountryCode GROUP BY T1.Name HAVING COUNT(*)  >  2\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode, language, sum(percent) from countrylanguage group by countrycode\n",
      "SELECT LANGUAGE ,  CountryCode ,  max(Percentage) FROM countrylanguage GROUP BY CountryCode\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select count(*) from countrylanguage where language = \"Spanish\" group by countrycode order by sum(percent) desc limit 1\n",
      "SELECT count(*) ,   max(Percentage) FROM countrylanguage WHERE LANGUAGE  =  \"Spanish\" GROUP BY CountryCode\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select countrycode from countrylanguage where language = \"Spanish\" group by countrycode order by sum(percent) desc limit 1\n",
      "SELECT CountryCode ,  max(Percentage) FROM countrylanguage WHERE LANGUAGE  =  \"Spanish\" GROUP BY CountryCode\n",
      "world_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.name, t2.orchestra from conductor as t1 join orchestra as t2 on t1.coach_id = t2.coach_id\n",
      "SELECT T1.Name ,  T2.Orchestra FROM conductor AS T1 JOIN orchestra AS T2 ON T1.Conductor_ID  =  T2.Conductor_ID\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "select t2.name from highschooler as t1 join friend as t2 on t1.id = t2.friend_id where t1.name = 'Kyle'\n",
      "SELECT T3.name FROM Friend AS T1 JOIN Highschooler AS T2 ON T1.student_id  =  T2.id JOIN Highschooler AS T3 ON T1.friend_id  =  T3.id WHERE T2.name  =  \"Kyle\"\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select t2.name from highschooler as t1 join friend as t2 on t1.id = t2.friend_id where t1.name = \"Kyle\"\n",
      "SELECT T3.name FROM Friend AS T1 JOIN Highschooler AS T2 ON T1.student_id  =  T2.id JOIN Highschooler AS T3 ON T1.friend_id  =  T3.id WHERE T2.name  =  \"Kyle\"\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select student_id from highschooler except select student_id from friend\n",
      "SELECT id FROM Highschooler EXCEPT SELECT student_id FROM Friend\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select student_id from highschooler except select student_id from friend\n",
      "SELECT id FROM Highschooler EXCEPT SELECT student_id FROM Friend\n",
      "network_1\n",
      "process_sql.get_sql() failed\n",
      "select t1.id from highschooler as t1 join friends as t2 on t1.id = t2.friend_id intersect select t1.id from highschooler as t1 join likes as t2 on t1.id = t2.liked_id\n",
      "SELECT student_id FROM Friend INTERSECT SELECT liked_id FROM Likes\n",
      "network_1\n",
      "process_sql.get_sql() failed\n"
     ]
    }
   ],
   "source": [
    "# Using original text (no ASR)\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "        \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "    \n",
    "    _pred_sql = Picard_predict(c['gold_question'], _db_id)\n",
    "    \n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    c['pred_sql'] = _pred_sql\n",
    "    c['score'] = _score\n",
    "    c['exact'] = _exact\n",
    "    c['exec'] = _exec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7874\n",
      "avg_exact = 0.6929\n",
      "avg_exec = 0.7148\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "# ## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "# print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in test_dataset],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_subset = random.sample(test_dataset, k=100)\n",
    "\n",
    "EvaluateSQL_full(plist=[d[0]['pred_sql'] for d in random_subset],\n",
    "                 glist=[d[0]['query'] for d in random_subset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in random_subset],\n",
    "                 etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End2end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '4.1.0.1_laststep'\n",
    "\n",
    "end2end_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "# test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_reranker.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 1034)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(end2end_pred_path, 'r') as f:\n",
    "    end2end_preds = [json.loads(l) for l in f]\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "    \n",
    "len(end2end_preds), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2067b6b7c9d41dfafda2172d32653bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3075), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "SELECT Max(performance.Official_ratings_(millions)), Min(performance.Share) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "_seen_ids = set()\n",
    "_first_cand_preds = []\n",
    "\n",
    "for i, p in tqdm(enumerate(end2end_preds), total=len(end2end_preds)):\n",
    "\n",
    "    # p = rewriter_ILM_preds[pred_idx]\n",
    "    # _o_idx = c['original_id']\n",
    "    # o = orig_dev_dataset[_o_idx]\n",
    "    # assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    # assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _o_idx = p['original_id']\n",
    "    if _o_idx in _seen_ids:\n",
    "        continue\n",
    "    else:\n",
    "        _seen_ids.add(_o_idx)\n",
    "        \n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    \n",
    "    # Debug \n",
    "    # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "    _pred_sql = p['pred_sql']\n",
    "    _gold_sql = p['gold_sql']\n",
    "    assert _gold_sql == o['query'], (_gold_sql, o['query'])\n",
    "    \n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "    \n",
    "    # Save prediction results \n",
    "    p['score'] = _score\n",
    "    p['exact'] = _exact\n",
    "    p['exec'] = _exec\n",
    "    \n",
    "    _first_cand_preds.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7660\n",
      "avg_exact = 0.4973\n",
      "avg_exec = 0.3620\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([p['score'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "_avg_exact_1st = sum([p['exact'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "_avg_exec_1st = sum([p['exec'] for p in _first_cand_preds]) / len(_first_cand_preds)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "# _std_1st = np.std([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']])\n",
    "\n",
    "print('avg = {:.4f}'.format(_avg_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '1.14.1.2'\n",
    "HUMAN_TEST = False\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    reranker_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_reranker.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "else:\n",
    "    reranker_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "    test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_reranker.json'\n",
    "    orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggreg with rewriter cands \n",
    "\n",
    "RERANKER_VERSION = '1.10.0.2'\n",
    "REWRITER_VERSION = '2.6.0.2t-2.6.0.2i'\n",
    "HUMAN_TEST = True\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    reranker_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/aggreg/output-{RERANKER_VERSION}-with-{REWRITER_VERSION}.json'\n",
    "    test_dataset_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/test_reranker_with_{REWRITER_VERSION}.json'\n",
    "    orig_dev_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "else:\n",
    "    reranker_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/aggreg/output-humantest-yshao-{RERANKER_VERSION}-with-{REWRITER_VERSION}.json'\n",
    "    test_dataset_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/human_test_yshao_reranker_with_{REWRITER_VERSION}.json'\n",
    "    orig_dev_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075, 547, 3075, 1034)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(reranker_pred_path, 'r') as f:\n",
    "    reranker_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(reranker_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['score_preds', 'question', 'original_id']),\n",
       " dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'span_ranges', 'original_id', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_exact', 'ratsql_pred_score', 'question_toks_edit_distance']))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker_preds[0].keys(), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9994f50f0dc74926b9ca1cc60bcfb1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_idx = 0\n",
    "\n",
    "ref_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Find the cand with highest score \n",
    "    d_preds = []\n",
    "    for _ in d:\n",
    "        p = reranker_preds[pred_idx]\n",
    "        d_preds.append(p)\n",
    "        pred_idx += 1\n",
    "    \n",
    "    _c_idx = np.argmax([p['score_preds'] for p in d_preds])\n",
    "    \n",
    "    c = d[_c_idx]\n",
    "    p = d_preds[_c_idx]\n",
    "    \n",
    "    for _c in d:\n",
    "        _c['is_reranker_selection'] = False\n",
    "    c['is_reranker_selection'] = True\n",
    "    \n",
    "    # Use the selected cand to proceed \n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "    \n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    _question = c['question']\n",
    "\n",
    "    if c['ratsql_pred_sql'] is not None:\n",
    "        # If SQL is already predicted, no need to predict again \n",
    "        _pred_sql = c['ratsql_pred_sql']\n",
    "    else:\n",
    "        # If SQL not yet predicted, do it \n",
    "        _pred_sql = Question(_question, _db_id)[0]['inferred_code']\n",
    "\n",
    "    _gold_sql = c['query']\n",
    "    _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "#     # Save the taggerILM raw outputs, for later aggregation \n",
    "#     c['pred_tags'] = p['rewriter_tags']\n",
    "#     c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "#     c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "    \n",
    "    # Save prediction results \n",
    "    # c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    # p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "    c['exact'] = p['exact'] = _exact\n",
    "    c['exec'] = p['exec'] = _exec\n",
    "\n",
    "    _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "    _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "    ref_list.append([_gold_question_toks])\n",
    "    hyp_list.append(_question_toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.7599 (std = 0.2977)\n",
      "avg_exact = 0.5027\n",
      "avg_exec = 0.3620\n",
      "BLEU = 0.7853\n"
     ]
    }
   ],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "_avg_exact_1st = sum([c['exact'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "_avg_exec_1st = sum([c['exec'] for d in test_dataset for c in d if c['is_reranker_selection']]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([c['score'] for d in test_dataset for c in d if c['is_reranker_selection']])\n",
    "\n",
    "## BLEU \n",
    "_bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                    hypotheses=hyp_list)\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "print(f'BLEU = {_bleu:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EvaluateSQL_full(plist=[c['pred_sql'] for d in test_dataset for c in d if c['is_reranker_selection']],\n",
    "                 glist=[d[0]['query'] for d in test_dataset],\n",
    "                 db_id_list=[d[0]['db_id'] for d in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non ASR\n",
    "if not HUMAN_TEST:\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "else:\n",
    "    test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "    \n",
    "# ASR\n",
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger-ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION: 2.16.0.0t\n",
      "Overall tagger accuracy = 38814/41776 = 0.9291\n",
      "Tagger P = 0.7764, R = 0.6817, F1 = 0.7260\n",
      "\n",
      "VERSION: 2.16.0.1t\n",
      "Overall tagger accuracy = 38782/41776 = 0.9283\n",
      "Tagger P = 0.7529, R = 0.6967, F1 = 0.7237\n",
      "\n",
      "VERSION: 2.16.0.2t\n",
      "Overall tagger accuracy = 38742/41776 = 0.9274\n",
      "Tagger P = 0.7446, R = 0.6858, F1 = 0.7140\n",
      "\n",
      "VERSION: 2.16.0.3t\n",
      "Overall tagger accuracy = 38893/41776 = 0.9310\n",
      "Tagger P = 0.7663, R = 0.6974, F1 = 0.7303\n",
      "\n",
      "VERSION: 2.16.0.4t\n",
      "Overall tagger accuracy = 38932/41776 = 0.9319\n",
      "Tagger P = 0.7640, R = 0.7072, F1 = 0.7345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tagger accuracy \n",
    "VERSION_LIST = [f'2.16.0.{v}t' for v in range(5)]\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    predict_fname = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "\n",
    "    with open(predict_fname, 'r') as f:\n",
    "        predicts = [json.loads(l) for l in f]\n",
    "    len(predicts)\n",
    "\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "\n",
    "    non_O_matches = 0\n",
    "    non_O_preds = 0\n",
    "    non_O_golds = 0\n",
    "\n",
    "    for p in predicts:\n",
    "        _pred_tags = p['tags_prediction']\n",
    "        _gold_tags = [t for t in p['gold_tags'] if t != 'O']\n",
    "        assert len(_pred_tags) == len(_gold_tags)\n",
    "\n",
    "        _cor = sum([_p == _g for _p, _g in zip(_pred_tags, _gold_tags)])\n",
    "        _total = len(_pred_tags)\n",
    "        assert -1e-8 < _cor / _total - p['tags_accuracy'] < 1e-8, f\"{_cor / _total}, {p['tags_accuracy']}\"\n",
    "\n",
    "        for _p, _g in zip(_pred_tags, _gold_tags):\n",
    "            if _p != 'O-KEEP':\n",
    "                non_O_preds += 1\n",
    "            if _g != 'O-KEEP':\n",
    "                non_O_golds += 1\n",
    "            if _p == _g and _p != 'O-KEEP':\n",
    "                non_O_matches += 1\n",
    "\n",
    "        correct_tags += _cor\n",
    "        total_tags += _total\n",
    "\n",
    "    tagger_prec = non_O_matches / non_O_preds\n",
    "    tagger_recall = non_O_matches / non_O_golds\n",
    "    tagger_F1 = 2 * tagger_prec * tagger_recall / (tagger_prec + tagger_recall)\n",
    "\n",
    "    print(f'VERSION: {VERSION}')\n",
    "    print(f'Overall tagger accuracy = {correct_tags}/{total_tags} = {correct_tags / total_tags:.4f}')\n",
    "    print(f'Tagger P = {tagger_prec:.4f}, R = {tagger_recall:.4f}, F1 = {tagger_F1:.4f}')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full tagger-ILM\n",
    "\n",
    "HUMAN_TEST = False\n",
    "ASR = 'Amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b555d94690e341b0bffff0dd8fe889e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.21.0.3i', max=547, style=ProgressStyle(de"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], [\"'GV\", \"'\"], [\"'SF\", \"'\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['Bosco'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['or'], ['who', 'id', 'is'], ['and'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.21.0.3i ====================\n",
      "avg_exact = 0.5338\n",
      "avg = 0.7753\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8755\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755ffed3d1b44aa4aa858a8d934bf5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.4t-2.21.0.4i', max=547, style=ProgressStyle(de"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['CV'], [\"'SF\", \"'\"], ['?']]\n",
      "\n",
      "==================== VERSION: 2.12.1.4t-2.21.0.4i ====================\n",
      "avg_exact = 0.5283\n",
      "avg = 0.7698\n",
      "avg_exec = 0.3729\n",
      "BLEU = 0.8769\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbefdbb278bd47b3a172e9e0669423fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.22.1.3i', max=547, style=ProgressStyle(de"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['and'], [\"'chervil\", \"'\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['code'], ['``', 'CWS', \"''\"]]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'U-DEL', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['id'], ['id'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['CV'], ['BK'], ['?']]\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.22.1.3i ====================\n",
      "avg_exact = 0.5302\n",
      "avg = 0.7767\n",
      "avg_exec = 0.3857\n",
      "BLEU = 0.8757\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3951915c7d6a4550884ccf79a4381cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.4t-2.22.1.4i', max=547, style=ProgressStyle(de"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-DEL', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['at']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['Retrieve', 'all']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['USPS'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'O-KEEP', 'B-EDIT', 'L-EDIT', 'U-EDIT', 'U-DEL', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['USPS'], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['ids'], ['contain'], ['``', 'Turon', \"''\"], ['?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'U-EDIT', 'O-KEEP', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['taught'], ['?']]\n",
      "\n",
      "==================== VERSION: 2.12.1.4t-2.22.1.4i ====================\n",
      "avg_exact = 0.5302\n",
      "avg = 0.7746\n",
      "avg_exec = 0.3839\n",
      "BLEU = 0.8808\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "VERSION_LIST = [f'2.12.1.{v}t-2.21.0.{v}i' for v in [3,4]]\n",
    "VERSION_LIST += [f'2.12.1.{v}t-2.22.1.{v}i' for v in [3,4]]\n",
    "# VERSION_LIST = [f'2.12.1.{v}t-2.16.{i}.{v}i' for i in [3,4] for v in range(5)]\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "        rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "\n",
    "    # len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)\n",
    "\n",
    "    ## Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    pred_idx = 0\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "\n",
    "    for d in tqdm(test_dataset, desc=f'VERSION {VERSION}'):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        # Debug \n",
    "        # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "        # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "\n",
    "        _tags = p['rewriter_tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "\n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _pred_sql = Question(_rewritten_question, _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "\n",
    "        _gold_sql = c['query']\n",
    "        _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        # Save the taggerILM raw outputs, for later aggregation \n",
    "        c['pred_tags'] = p['rewriter_tags']\n",
    "        c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "        c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "\n",
    "        # Save prediction results \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        c['exec'] = p['exec'] = _exec\n",
    "\n",
    "        _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "\n",
    "    print('='*20, f'VERSION: {VERSION}', '='*20)\n",
    "    print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "    # print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "    print('avg = {:.4f}'.format(_avg_1st))\n",
    "    print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print('='*55)\n",
    "    \n",
    "    # Non ASR\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "    # ASR\n",
    "    # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "    # Mixed\n",
    "    # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/{VERSION}.json'\n",
    "\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non ASR\n",
    "# if not HUMAN_TEST:\n",
    "#     if ASR == 'AssemblyAI':\n",
    "#         test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "#     else:\n",
    "#         test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "# else:\n",
    "#     test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "# # ASR\n",
    "# # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-asr-test-save/{VERSION}.json'\n",
    "# # Mixed\n",
    "# # test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-mixed-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freeze/Modify POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8ec7c06e674bc1b57102ea9943d3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.12.3.3i, POS SCONJ', max=547, style=Progr"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.12.3.3i ====================\n",
      "avg_exact = 0.5356\n",
      "avg = 0.7728\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8746\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785a1ba64f304e75b05a7c52ed81f7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.12.3.3i, POS PART', max=547, style=Progre"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.12.3.3i ====================\n",
      "avg_exact = 0.5356\n",
      "avg = 0.7728\n",
      "avg_exec = 0.3784\n",
      "BLEU = 0.8746\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7689cea0577422886b4073d1d32d016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.12.3.3i, POS ADV', max=547, style=Progres"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: 2.12.1.3t-2.12.3.3i ====================\n",
      "avg_exact = 0.5356\n",
      "avg = 0.7736\n",
      "avg_exec = 0.3803\n",
      "BLEU = 0.8742\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "VERSION_LIST = [f'2.12.1.{v}t-2.12.3.{v}i' for v in [3]]\n",
    "# VERSION_LIST += [f'2.12.1.{v}t-2.16.5.{v}i' for v in [3,4]]\n",
    "# VERSION_LIST = [f'2.12.1.{v}t-2.16.{i}.{v}i' for i in [3,4] for v in range(5)]\n",
    "\n",
    "# POS_LIST = [\"PUNCT\", \"NUM\", \"CCONJ\", \"PRON\", \"AUX\", \"VERB\", \"NOUN\", \"DET\", \"ADP\", \"ADJ\", \"PROPN\"]\n",
    "POS_LIST = [\"SCONJ\", \"PART\", \"ADV\"]\n",
    "\n",
    "GROUP_LIST = [(_v, _p) for _v in VERSION_LIST for _p in POS_LIST]\n",
    "\n",
    "for VERSION, POS in GROUP_LIST:\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "        rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "\n",
    "    # len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)\n",
    "\n",
    "    ## Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    pred_idx = 0\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "\n",
    "    for d in tqdm(test_dataset, desc=f'VERSION {VERSION}, POS {POS}'):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        # Debug \n",
    "        # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "        # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "\n",
    "        _tags = p['rewriter_tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "\n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq_freeze_POS(\n",
    "            _tags,\n",
    "            _rewrite_seq,\n",
    "            _question_toks,\n",
    "            freeze_POS=POS,\n",
    "            nlp=nlp)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _pred_sql = Question(_rewritten_question, _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "\n",
    "        _gold_sql = c['query']\n",
    "        _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        # Save the taggerILM raw outputs, for later aggregation \n",
    "        c['pred_tags'] = p['rewriter_tags']\n",
    "        c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "        c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "\n",
    "        # Save prediction results \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        c['exec'] = p['exec'] = _exec\n",
    "\n",
    "        _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "\n",
    "    print('='*20, f'VERSION: {VERSION}', '='*20)\n",
    "    print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "    # print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "    print('avg = {:.4f}'.format(_avg_1st))\n",
    "    print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print('='*55)\n",
    "    \n",
    "    # Non ASR\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}-freeze={POS}.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}-freeze={POS}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}-freeze={POS}.json'\n",
    "\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d439a3dfa34a928db1aa78112a0146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.12.3.3i, POS SCONJ', max=547, style=Progr"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: 2.12.1.3t-2.12.3.3i ====================\n",
      "avg_exact = 0.4570\n",
      "avg = 0.7273\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8010\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50da25cb1f374021ac27fa6a06fa922d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.12.3.3i, POS PART', max=547, style=Progre"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== VERSION: 2.12.1.3t-2.12.3.3i ====================\n",
      "avg_exact = 0.4570\n",
      "avg = 0.7273\n",
      "avg_exec = 0.3400\n",
      "BLEU = 0.8010\n",
      "=======================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b331c1c55a4a62b6602a8146db66e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='VERSION 2.12.1.3t-2.12.3.3i, POS ADV', max=547, style=Progres"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "==================== VERSION: 2.12.1.3t-2.12.3.3i ====================\n",
      "avg_exact = 0.4552\n",
      "avg = 0.7258\n",
      "avg_exec = 0.3382\n",
      "BLEU = 0.8014\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "VERSION_LIST = [f'2.12.1.{v}t-2.12.3.{v}i' for v in [3]]\n",
    "# VERSION_LIST += [f'2.12.1.{v}t-2.16.5.{v}i' for v in [3,4]]\n",
    "# VERSION_LIST = [f'2.12.1.{v}t-2.16.{i}.{v}i' for i in [3,4] for v in range(5)]\n",
    "\n",
    "# POS_LIST = [\"PUNCT\", \"NUM\", \"CCONJ\", \"PRON\", \"AUX\", \"VERB\", \"NOUN\", \"DET\", \"ADP\", \"ADJ\", \"PROPN\"]\n",
    "POS_LIST = [\"SCONJ\", \"PART\", \"ADV\"]\n",
    "\n",
    "GROUP_LIST = [(_v, _p) for _v in VERSION_LIST for _p in POS_LIST]\n",
    "\n",
    "for VERSION, POS in GROUP_LIST:\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{}.json'.format(VERSION)\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        rewriter_ILM_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    with open(rewriter_ILM_pred_path, 'r') as f:\n",
    "        rewriter_ILM_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "\n",
    "    # len(rewriter_ILM_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)\n",
    "\n",
    "    ## Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    pred_idx = 0\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "\n",
    "    for d in tqdm(test_dataset, desc=f'VERSION {VERSION}, POS {POS}'):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        # Debug \n",
    "        # assert c['rewriter_tags'] == p['rewriter_tags'][:len(c['rewriter_tags'])], f\"{c['rewriter_tags']}\\n{p['rewriter_tags']}\\nShould raise\"\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']  # For previous taggerILM joint model \n",
    "        # _tags = p['tags']  # Before adding align_tags (when 'tags' refers to 'rewriter_tags')\n",
    "\n",
    "        _tags = p['rewriter_tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "\n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq_modify_POS(\n",
    "            _tags,\n",
    "            _rewrite_seq,\n",
    "            _question_toks,\n",
    "            modify_POS=POS,\n",
    "            nlp=nlp)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _pred_sql = Question(_rewritten_question, _db_id, model_dict=model_dicts['orig'])[0]['inferred_code']\n",
    "\n",
    "        _gold_sql = c['query']\n",
    "        _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        # Save the taggerILM raw outputs, for later aggregation \n",
    "        c['pred_tags'] = p['rewriter_tags']\n",
    "        c['pred_ILM'] = p['rewrite_seq_prediction']\n",
    "        c['pred_ILM_cands'] = p['rewrite_seq_prediction_cands']\n",
    "\n",
    "        # Save prediction results \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        c['exec'] = p['exec'] = _exec\n",
    "\n",
    "        _rewritten_question_toks = [_t.lower() for _t in _rewritten_question_toks]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "\n",
    "    print('='*20, f'VERSION: {VERSION}', '='*20)\n",
    "    print('avg_exact = {:.4f}'.format(_avg_exact_1st))\n",
    "    # print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))\n",
    "    print('avg = {:.4f}'.format(_avg_1st))\n",
    "    print('avg_exec = {:.4f}'.format(_avg_exec_1st))\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print('='*55)\n",
    "    \n",
    "    # Non ASR\n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}-modify={POS}.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}-modify={POS}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}-modify={POS}.json'\n",
    "\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading from predicted file (only 1st cand is predicted!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '3.6.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using EvaluateSQL_full \n",
    "\n",
    "# tables_json = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "# kmaps = evaluation.build_foreign_key_map_from_json(tables_json)\n",
    "\n",
    "plist = [d[0]['pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reranker output file \n",
    "VERSION = '1.10.0.2'\n",
    "\n",
    "test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "with open(test_output_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "len(test_dataset), test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [c['ratsql_pred_sql'] for d in test_dataset for c in d if c['is_reranker_selection'] == 1]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plist = [d[0]['ratsql_pred_sql'] for d in test_dataset]\n",
    "glist = [d[0]['query'] for d in test_dataset]\n",
    "db_id_list = [d[0]['db_id'] for d in test_dataset]\n",
    "assert len(plist) == len(glist) == len(db_id_list)\n",
    "\n",
    "EvaluateSQL_full(glist=glist,\n",
    "                 plist=plist,\n",
    "                 db_id_list=db_id_list,\n",
    "                 kmaps=kmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using all ASR candidates (no longer in use)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags_prediction']\n",
    "        _tags = p['tags']\n",
    "        _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation process with oracle tags (no longer in use for version>=2.3.0)\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _tags = p['gold_tags']\n",
    "        _rewrite_seq = p['oracle_tags_rewrite_seq_prediction']\n",
    "        _question_toks = c['question_toks']\n",
    "        \n",
    "        _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "        \n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question'] = _rewritten_question\n",
    "        c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql'] = _pred_sql\n",
    "        c['oracle_tags_score'] = p['oracle_tags_score'] = _score\n",
    "\n",
    "        pred_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['oracle_tags_score'] for p in rewriter_ILM_preds]) / len(rewriter_ILM_preds))\n",
    "print(sum([c['oracle_tags_score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_oracle_avg_1st = sum([d[0]['oracle_tags_score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_oracle_std_1st = np.std([d[0]['oracle_tags_score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_oracle_avg_1st, _oracle_std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['tag_prediction'] = list(zip(p['question'].split(' '), p['tags_prediction']))\n",
    "        _pred_c['rewrite_seq'] = []\n",
    "        for t in p['rewrite_seq_prediction']:\n",
    "            _pred_c['rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['rewritten_question'] = p['rewritten_question']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_tags'] = list(zip(p['question'].split(' '), p['gold_tags']))\n",
    "        _pred_c['oracle_tags_rewrite_seq'] = []\n",
    "        for t in p['oracle_tags_rewrite_seq_prediction']:\n",
    "            _pred_c['oracle_tags_rewrite_seq'].append(t)\n",
    "            if t == '@end@': break\n",
    "        _pred_c['oracle_tags_rewritten_question'] = p['oracle_tags_rewritten_question']\n",
    "        _pred_c['oracle_tags_pred_sql'] = p['oracle_tags_pred_sql']\n",
    "        _pred_c['oracle_tags_score'] = p['oracle_tags_score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/test-prediction-{}.json'.format(VERSION), 'w') as f:\n",
    "    json.dump(test_pred_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset file with predictions \n",
    "\n",
    "with open('./output/pred-{}.json'.format(VERSION), 'r') as f:\n",
    "    test_pred_dataset = json.load(f)\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis \n",
    "orig_dev_preds_path = './output/dev_output.txt'\n",
    "\n",
    "with open(orig_dev_preds_path, 'r') as f:\n",
    "    orig_dev_preds = [l.strip() for l in f.readlines()]\n",
    "\n",
    "len(orig_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for d in test_dataset[5::40]:\n",
    "#     print('DB:', d[0]['db_id'])\n",
    "#     print('ASR question:\\t\\t', d[0]['question'])\n",
    "#     print('Rewritten question:\\t', d[0]['rewritten_question'])\n",
    "#     print('Gold question:\\t\\t', d[0]['gold_question'])\n",
    "#     print('ASR-q Pred SQL:\\t\\t', d[0]['ratsql_pred_sql'])\n",
    "#     print('Rewritten-q Pred SQL:\\t', d[0]['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', orig_dev_preds[d[0]['original_id']])\n",
    "#     print('Gold SQL:\\t\\t', d[0]['query'])\n",
    "#     print('Score:', d[0]['score'])\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_samples = [c for d in test_pred_dataset for c in d]\n",
    "for i, c in list(enumerate(test_pred_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['ASR_question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "    print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['gold_sql'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ids = [8, 96, 272, 448, 1416, 1592, 1680, 1856, 2120, 2296, 2384, 2560, 2824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating (Amazon): VERSION = 3.12.1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbadc38d54ef4598b38e1fb716ad31a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERSION 3.12.1.1:\n",
      "avg = 0.7410 (std = 0.3078)\n",
      "avg_exact = 0.4826\n",
      "avg_exec = 0.3638\n",
      "BLEU = 0.8532\n",
      "\n",
      "Evaluating (Amazon): VERSION = 3.12.1.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb02e39911f48f08c1f045f67cbcb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Max(performance.Share), Min(performance.Official_ratings_(millions)) FROM performance WHERE performance.Type != 'terminal'\n",
      "SELECT max(SHARE) ,  min(SHARE) FROM performance WHERE TYPE != \"Live final\"\n",
      "orchestra\n",
      "process_sql.get_sql() failed\n",
      "\n",
      "VERSION 3.12.1.2:\n",
      "avg = 0.7478 (std = 0.3037)\n",
      "avg_exact = 0.4899\n",
      "avg_exec = 0.3675\n",
      "BLEU = 0.8566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Batch evaluating \n",
    "\n",
    "VERSION_LIST = ['3.12.1.1', '3.12.1.2']\n",
    "HUMAN_TEST = False\n",
    "ASR = 'Amazon'\n",
    "\n",
    "for VERSION in VERSION_LIST:\n",
    "    print(f'Evaluating ({ASR}){\" (human test)\" if HUMAN_TEST else \"\"}: VERSION = {VERSION}')\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/output-{VERSION}.json'\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/Assembly_transcribe/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "        else:\n",
    "            rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "            test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "            orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "    else:\n",
    "        # human test \n",
    "        rewriter_s2s_pred_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-humantest-yshao-{}.json'.format(VERSION)\n",
    "        test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_rewriter.json'\n",
    "        orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test.json'\n",
    "\n",
    "    \n",
    "    with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "        rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "    with open(test_dataset_path, 'r') as f:\n",
    "        test_dataset = json.load(f)\n",
    "    with open(orig_dev_path, 'r') as f:\n",
    "        orig_dev_dataset = json.load(f)\n",
    "        \n",
    "    # Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "    ref_list = []\n",
    "    hyp_list = []\n",
    "    \n",
    "    pred_idx = 0\n",
    "\n",
    "    for d in tqdm(test_dataset):\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "\n",
    "        c = d[0]\n",
    "\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        # _tags = p['tags']\n",
    "        # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "        # _question_toks = c['question_toks']\n",
    "\n",
    "        # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "        # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "\n",
    "        if _rewritten_question == '':\n",
    "            print(f'_rewritten_question is empty')\n",
    "            _pred_sql = ''\n",
    "            _gold_sql = c['query']\n",
    "            _exact = _score = _exec = 0\n",
    "        else:\n",
    "            _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "            _gold_sql = c['query']\n",
    "            _exact, _score, _exec = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "        c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "        c['exact'] = p['exact'] = _exact\n",
    "        c['exec'] = p['exec'] = _exec\n",
    "        \n",
    "        # For BLEU \n",
    "        _rewritten_question_toks = [_t.lower() for _t in p['s2s_prediction']]\n",
    "        _question_toks = [_t.lower() for _t in c['question_toks']]\n",
    "        _gold_question_toks = [_t.lower() for _t in c['gold_question_toks']]\n",
    "\n",
    "        ref_list.append([_gold_question_toks])\n",
    "        hyp_list.append(_rewritten_question_toks)\n",
    "\n",
    "        pred_idx += len(d)\n",
    "\n",
    "    # Only using the 1st candidate to rewrite \n",
    "    _avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exact_1st = sum([d[0]['exact'] for d in test_dataset]) / len(test_dataset)\n",
    "    _avg_exec_1st = sum([d[0]['exec'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "    ## Std-dev (1st cand only)\n",
    "    _std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "    \n",
    "    ## BLEU \n",
    "    _bleu = corpus_bleu(list_of_references=ref_list,\n",
    "                        hypotheses=hyp_list)\n",
    "    \n",
    "    print(f'VERSION {VERSION}:')\n",
    "    print(f'avg = {_avg_1st:.4f} (std = {_std_1st:.4f})')\n",
    "    print(f'avg_exact = {_avg_exact_1st:.4f}')\n",
    "    print(f'avg_exec = {_avg_exec_1st:.4f}')\n",
    "    print(f'BLEU = {_bleu:.4f}')\n",
    "    print()\n",
    "    \n",
    "    if not HUMAN_TEST:\n",
    "        if ASR == 'AssemblyAI':\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/AssemblyAI/ratsql-test-save/{VERSION}.json'\n",
    "        else:\n",
    "            test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "    else:\n",
    "        test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{VERSION}.json'\n",
    "        \n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single version, not used \n",
    "\n",
    "VERSION = '3.3.0.0'\n",
    "\n",
    "rewriter_s2s_pred_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/output-{VERSION}.json'\n",
    "test_dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "orig_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rewriter_s2s_pred_path, 'r') as f:\n",
    "    rewriter_preds = [json.loads(l) for l in f.readlines()]\n",
    "with open(test_dataset_path, 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "with open(orig_dev_path, 'r') as f:\n",
    "    orig_dev_dataset = json.load(f)\n",
    "\n",
    "len(rewriter_preds), len(test_dataset), sum([len(d) for d in test_dataset]), len(orig_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in rewriter_preds[3::300]:\n",
    "    print(p['question'])\n",
    "    print(' '.join(p['s2s_prediction']))\n",
    "    print(' '.join(p['gold_rewrite_seq_s2s'][1:-1]))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation: only using the 1st ASR candidate\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    c = d[0]\n",
    "    \n",
    "    p = rewriter_preds[pred_idx]\n",
    "    _o_idx = c['original_id']\n",
    "    o = orig_dev_dataset[_o_idx]\n",
    "    assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "    assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "\n",
    "    _db_id = o['db_id']\n",
    "\n",
    "    # _tags = p['tags']\n",
    "    # _rewrite_seq = p['rewrite_seq_prediction']\n",
    "    # _question_toks = c['question_toks']\n",
    "\n",
    "    # _rewritten_question_toks = Postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)\n",
    "    # _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "    \n",
    "    _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "    \n",
    "    if _rewritten_question == '':\n",
    "        print(f'_rewritten_question is empty')\n",
    "        _pred_sql = ''\n",
    "        _gold_sql = c['query']\n",
    "        _score = 0\n",
    "    else:\n",
    "        _pred_sql = Question(_rewritten_question, _db_id)[0]['inferred_code']\n",
    "        _gold_sql = c['query']\n",
    "        _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "\n",
    "    c['rewritten_question'] = p['rewritten_question'] = _rewritten_question\n",
    "    c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "    p['gold_sql'] = _gold_sql\n",
    "    c['score'] = p['score'] = _score\n",
    "\n",
    "    pred_idx += len(d)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Actual (full) evaluation process \n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    for c in d:\n",
    "        p = rewriter_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        pred_idx += 1\n",
    "        if 'score' in c:\n",
    "            continue  # already inferred  \n",
    "        \n",
    "        _db_id = o['db_id']\n",
    "\n",
    "        _rewritten_question = ' '.join(p['s2s_prediction'])\n",
    "        _pred_result = Question(_rewritten_question, _db_id)\n",
    "        \n",
    "        _gold_sql = c['query']\n",
    "        \n",
    "        if len(_pred_result) == 0:\n",
    "            print(_db_id, _rewritten_question, '-- no predictiction')\n",
    "            _pred_sql = ''\n",
    "            _score = 0\n",
    "        else:\n",
    "            _pred_sql = _pred_result[0]['inferred_code']\n",
    "            _score = EvaluateSQL(_pred_sql, _gold_sql, _db_id)\n",
    "        \n",
    "        c['rewritten_question'] = _rewritten_question\n",
    "        c['pred_sql'] = p['pred_sql'] = _pred_sql\n",
    "        p['gold_sql'] = _gold_sql\n",
    "        c['score'] = p['score'] = _score\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the candidates to rewrite \n",
    "print(sum([p['score'] for p in rewriter_preds]) / len(rewriter_preds))\n",
    "print(sum([c['score'] for d in test_dataset for c in d]) / sum([len(d) for d in test_dataset]))\n",
    "\n",
    "# Only using the 1st candidate to rewrite \n",
    "_avg_1st = sum([d[0]['score'] for d in test_dataset]) / len(test_dataset)\n",
    "\n",
    "## Std-dev (1st cand only)\n",
    "_std_1st = np.std([d[0]['score'] for d in test_dataset])\n",
    "\n",
    "print('avg = {:.4f} (std = {:.4f})'.format(_avg_1st, _std_1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results in a single dataset obj \n",
    "\n",
    "test_pred_dataset = []\n",
    "\n",
    "pred_idx = 0\n",
    "\n",
    "for d in tqdm(test_dataset):\n",
    "    _pred_d = []\n",
    "    \n",
    "    for c in d:\n",
    "        p = rewriter_ILM_preds[pred_idx]\n",
    "        _o_idx = c['original_id']\n",
    "        o = orig_dev_dataset[_o_idx]\n",
    "        assert ' '.join(c['question_toks']) == p['question'], (' '.join(c['question_toks']), p['question'])\n",
    "        assert c['gold_question_toks'] == o['question_toks'], (c['gold_question_toks'], o['question_toks'])\n",
    "        \n",
    "        _pred_c = dict()\n",
    "        \n",
    "        _pred_c['ASR_question'] = p['question']\n",
    "        _pred_c['ASR_question_pred_sql'] = c['ratsql_pred_sql']\n",
    "        \n",
    "        _pred_c['gold_question'] = c['gold_question']\n",
    "        # _pred_c['gold_question_pred_sql'] = orig_dev_preds[c['original_id']]\n",
    "        \n",
    "        _pred_c['rewritten_question'] = p['s2s_prediction']\n",
    "        _pred_c['pred_sql'] = p['pred_sql']\n",
    "        _pred_c['score'] = p['score']\n",
    "        \n",
    "        _pred_c['gold_sql'] = c['query']\n",
    "        \n",
    "        _pred_d.append(_pred_c)\n",
    "\n",
    "        pred_idx += 1\n",
    "    \n",
    "    test_pred_dataset.append(_pred_d)\n",
    "\n",
    "len(test_pred_dataset), sum([len(d) for d in test_pred_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{VERSION}.json'\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [c for d in test_dataset for c in d]\n",
    "for i, c in list(enumerate(test_samples))[8::88]:\n",
    "    print('-'*30, 'ID = {}'.format(i), '-'*30)\n",
    "    print('ASR question:\\t\\t', c['question'])\n",
    "    print('Rewritten question:\\t', c['rewritten_question'])\n",
    "    print('Gold question:\\t\\t', c['gold_question'])\n",
    "    print('Rewritten-q Pred SQL:\\t', c['pred_sql'])\n",
    "#     print('Gold-q Pred SQL:\\t', c['gold_question_pred_sql'])\n",
    "    print('Gold SQL:\\t\\t', c['query'])\n",
    "    print('Score:', c['score'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewriter_ILM_preds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dev_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite_seq postprocessing, to get the rewritten question \n",
    "\n",
    "_idx = 154\n",
    "\n",
    "_tags = rewriter_ILM_preds[_idx]['tags_prediction']\n",
    "_rewrite_seq = rewriter_ILM_preds[_idx]['rewrite_seq_prediction']\n",
    "_question_toks = rewriter_ILM_preds[_idx]['question'].split(' ')\n",
    "_tags, _rewrite_seq, _question_toks\n",
    "\n",
    "postprocess_rewrite_seq(_tags, _rewrite_seq, _question_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'concert_singer'\n",
    "g_str = 'SELECT count(*) FROM singer'\n",
    "p_str = \"SELECT Count(DISTINCT singer.Name) FROM singer WHERE singer.Name = 'terminal'\"\n",
    "\n",
    "db, p_str, g_str, EvaluateSQL(p_str, g_str, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 41112)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/train/train_reranker.json') as f:\n",
    "    _train = json.load(f)\n",
    "len(_train), sum([len(d) for d in _train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 623.44 MiB, increment: -2.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
